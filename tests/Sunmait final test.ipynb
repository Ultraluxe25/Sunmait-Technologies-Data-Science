{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efeccd7b-704d-4337-83c4-40381b485768",
   "metadata": {},
   "source": [
    "### 1. Как работает расщепление на основе энтропии в решающих деревьях?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ff02b-7806-4e07-9690-3ea21f56cc4b",
   "metadata": {},
   "source": [
    "В решающем дереве есть три критерия разделения (по крайней мере в Scikit-Learn) - Джини, энтропия и лог лосс. Энтропия - это мера хаоса и неопределенности. Соответственно каждый сплит дерева должен в своих поддеревьях эту меру уменьшать, делая данные более предсказуемыми. Расщепление будет происходить по тому признаку, который имеет наивысший Information gain - разница значения энтропии до разделения и после"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790a38b-97dd-4aec-b724-76457284520d",
   "metadata": {},
   "source": [
    "### 2. Опишите как работает метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8636031-c156-4c6b-a7f4-8239a16142a0",
   "metadata": {},
   "source": [
    "SVM (В Scikit-Learn SVC) использует гиперплоскость для разделения признакового пространства на два подпространства (чаще для задач бинарной классификации используется, но можно и для регрессии). Цель метода в максимизации отступа разделяющей гиперплоскости от ближайших объектов обоих классов (опорных векторов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9b4ae-0eb8-4f20-8f3b-8d0f7a15482e",
   "metadata": {},
   "source": [
    "### 3. Для чего нужны ядерные (kernel) функции в SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aed7a0-d547-4bf5-a183-3bfaef67130e",
   "metadata": {},
   "source": [
    "Если признаковое пространство большое, то линейной гиперплоскостью их описать не получится (можем столкнуться с проблемой недообучения). Поэтому приходится пользоваться полиномами, которые позволяют описать нелинейные зависимости в данных, увеличив margin. Ядерные функции (линейная, полиномиальная и др.) помогают в этом, но могут и привести к проблеме высокой размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433c3ae-d8df-40ac-8048-791cb1aaa121",
   "metadata": {},
   "source": [
    "### 4. Для чего нужен параметр k в  kNN алгоритме?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2b3dd-7893-4567-907b-91cb319a6f65",
   "metadata": {},
   "source": [
    "k - это гиперпараметр, который отвечает за количество ближайших объектов (\"соседей\"), расстояние до которых будет вычесленно до принятия решений (голосование большинством или усреднение). Маленькое значение k ведет к переобучению, а большое к недообучению, поэтому настраиваем с помощью Optuna тщательно. Метрики расстояния могут быть: Евклидова, Манхэттенская, косинусная, Махаланобиса, Миньковского (default=’minkowski’ в Scikit-Learn) и др. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbca70-c891-4076-8ad0-e692da821e46",
   "metadata": {},
   "source": [
    "### 5. Назовите три основных строительных блока алгоритмов обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5570e1-5809-447e-803f-3c5cc8644975",
   "metadata": {},
   "source": [
    "1) Подготовка данных (предобработка датасета, его подбор, а также EDA)\n",
    "2) Выбор алгоритма обучения (или группы), для решения задачи, правильную функции потерь и оптимизатор для конкретного типа задач\n",
    "3) Обучение модели, подбор гиперпараметров с помощью кросс-валидации и её тестирование на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82a007-bde5-4f35-b231-73cce25ac298",
   "metadata": {},
   "source": [
    "### 6. Как инициализация параметров может повлиять на решение, найденное градиентным спуском в сложных моделях, таких как нейронные сети?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c15d68-6141-468b-8160-7c84cc5d09a5",
   "metadata": {},
   "source": [
    "При малых значениях весов градиент (вектор наискорейшего роста, а антиградиент скорейшего спуска, нам нужен антиградиент для минимизации функции потерь) может затухнуть и веса почти прекратят обновляться, а при больших значениях произойдет взрыв градиента. Для решения этих проблем существуют подходы для инициализации вроде Xavier (Glorot) или He."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc21b2-054e-4178-9bfa-a3be3e173d9a",
   "metadata": {},
   "source": [
    "### 7. Что такое проектирование признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42803b-3c46-4f8b-9fdb-2762262544d0",
   "metadata": {},
   "source": [
    "Feature Engineering - это создание новых признаков на основе имеющихся, используется в классическом машинном обучении, т.к. в глубоком обучение этим занимается не инженер, а нейронные сети. Чаще всего именно знания доменной области позволяет ML-инженером создать новый признак из нелинейной комбинации имеющихся, чтобы увеличить обобщающую способность модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a14c93-ed9f-4d33-9ca3-064ef3d5ef8a",
   "metadata": {},
   "source": [
    "### 8. Как следует преобразовывать категориальные признаки, чтобы избежать путаницы алгоритма обучения, особенно когда порядок значений признака не важен?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf660c4-65a1-41e7-8196-4c70da0f106d",
   "metadata": {},
   "source": [
    "Необходимо закодировать такой признак и лучше в числовой формат. Например One-hot encoding может, расширив признаковое пространство, создать n-1 новый столбец, в каждом из которых появится отдельное значение признака и при его совпадении с образцом в ячейке будет 1, а иначе 0. Есть еще Label Encoding (похожа на ранги и в этом её минус), Target Encoding и другие. По умолчанию моделям ML тяжело работать с категориальными значениями и их нужно переводить в числовые, но такая библиотека как CatBoost способна самостоятельно обрабатывать категориальные признаки - это её особенность среди градиентных бустингов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bdf7fa-b34c-4de7-8877-de070f616c15",
   "metadata": {},
   "source": [
    "### 9. Что такое стандартизация данных?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac1d49-218f-4395-b6cd-37c1004b700e",
   "metadata": {},
   "source": [
    "Чтобы слишком большие значения признаков или слишком маленькие (по масштабу) могли влиять на модель  их нужно загнать в рами диапозона [0, 1], так что среднее значение новых значений признаков станет 0, а средне квадратичное отклонение 1. Стандартизация увеличивает скорость работы некоторых алгоритмов. Проводится с помощью (X_i - X_сред) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2f34f-d545-4e83-a4f3-2667a69dad09",
   "metadata": {},
   "source": [
    "### 10. Объясните, зачем нужны три набора данных: обучающий, контрольный и тестовый."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b4eb3-112b-438d-b523-1eb4874bbeec",
   "metadata": {},
   "source": [
    "На тренировочном наборе модель обучается находить закономерности и важные зависимости в данных, на валидационной модель настраивает свои гиперпараметры (особенно хорошо при кросс-валидации), а на тестовой проверяется качество обученной модели на метриках"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb081d31-39e6-461c-8cb1-510b97bb8619",
   "metadata": {},
   "source": [
    "### 11. Что такое поиск по сетке, и как он используется для настройки гиперпараметров алгоритма?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cd37e-87f8-443c-9de7-bcae5dc25070",
   "metadata": {},
   "source": [
    "GridSearchCV (в Scikit-Learn) - это подход для подбора гиперпараметров, который среди всех возможных комбинаций значений гиперпараметров, указанных инженером, выбирает ту, которая дает наилучший результат на метрике. Этот метод очень неэффективен на больших размерах данных или на большой комбинации значений признаков - очень затратен по времени. На практике все сейчас используют Optuna или Hyperopt для подбора гиперпараметров."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
